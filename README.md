# ğŸ“š Amazon Review Summarization using NLP ğŸš€  

ğŸ“ **Transforming lengthy customer reviews into concise, meaningful summaries using state-of-the-art NLP techniques!**  

---

## ğŸŒŸ Project Overview  
This project focuses on **abstractive text summarization** of **Amazon Fine Food reviews** using **Deep Learning and Transformer-based models**. The goal is to generate **coherent, high-quality summaries** that help in understanding customer feedback efficiently.  

ğŸ“Œ **Key Features:**  
- ğŸ›  **Data Preprocessing & Cleaning** â€“ Tokenization, stopword removal, and text normalization.  
- ğŸ¤– **Abstractive Summarization** â€“ Implemented using **BART, T5, and custom Seq2Seq models with attention**.  
- ğŸ“Š **Evaluation Metrics** â€“ Used **ROUGE and BLEU scores** to measure summary quality.  
- ğŸš€ **Scalable & Automated Pipeline** â€“ Designed for easy integration with real-world datasets.  

---

## ğŸ“‚ Dataset  
ğŸ”— The dataset used in this project is **[Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)** from Kaggle.  
It contains **500,000+ customer reviews**, providing an excellent testbed for NLP-based summarization.  

**Dataset Features:**  
âœ… **Review Text** â€“ Full customer reviews  
âœ… **Summary** â€“ Human-generated short summaries  
âœ… **Product & User Information**  

---

## ğŸ›  Technologies Used  
âœ… **Python** ğŸ  
âœ… **TensorFlow & Keras** â€“ Deep learning framework  
âœ… **Hugging Face Transformers** â€“ Pretrained BART & T5 models  
âœ… **NLTK & spaCy** â€“ Text preprocessing and tokenization  
âœ… **Pandas & NumPy** â€“ Data manipulation  
âœ… **ROUGE & BLEU Scores** â€“ Summary evaluation  

---

## ğŸš€ How to Run the Project  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/yourusername/amazon-review-summarization.git
cd amazon-review-summarization
ğŸ”¬ Model Performance & Evaluation
The models were evaluated using ROUGE and BLEU scores, which measure the similarity between generated and human-written summaries.

## ğŸ”¬ Model Performance & Evaluation  
The models were evaluated using **ROUGE and BLEU scores**, which measure the similarity between generated and human-written summaries.  

### ğŸ“ˆ Results:  

| Model                  | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU  |
|------------------------|---------|---------|---------|-------|
| **BART**              | **0.166** | **0.055** | **0.159** | **0.025** |
| **T5**                | 0.155   | 0.045   | 0.151   | 0.024 |
| **Seq2Seq + Attention** | 0.103   | 0.007   | 0.096   | 0.017 |

âœ… **BART outperformed other models, delivering the best-quality summaries!** ğŸš€  
